% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/validate_fn.R
\name{validate_fn}
\alias{validate_fn}
\title{Validate custom model functions on a test set}
\usage{
validate_fn(
  train_data,
  formulas,
  type,
  model_fn,
  predict_fn,
  test_data = NULL,
  preprocess_fn = NULL,
  preprocess_once = FALSE,
  hyperparameters = NULL,
  partitions_col = ".partitions",
  cutoff = 0.5,
  positive = 2,
  metrics = list(),
  rm_nc = FALSE,
  parallel = FALSE,
  verbose = TRUE
)
}
\arguments{
\item{train_data}{Data frame.

 Can contain a grouping factor for identifying partitions - as made with
 \code{\link[groupdata2:partition]{groupdata2::partition()}}.
 See \code{partitions_col}.}

\item{formulas}{Model formulas as strings. (Character)

 Will be converted to \code{\link[stats:formula]{formula}} objects
 before being passed to \code{model_fn}.

 E.g. \code{c("y~x", "y~z")}.

 Can contain random effects.

 E.g. \code{c("y~x+(1|r)", "y~z+(1|r)")}.}

\item{type}{Type of evaluation to perform:

 \code{"gaussian"} for regression (like linear regression).

 \code{"binomial"} for binary classification.

 \code{"multinomial"} for multiclass classification.}

\item{model_fn}{Model function that returns a fitted model object.
 Will usually wrap an existing model function like \code{\link[e1071:svm]{e1071::svm}}
 or \code{\link[nnet:multinom]{nnet::multinom}}.

 Must have the following function arguments:

 \code{function(train_data, formula,}

 \verb{         }\code{hyperparameters)}}

\item{predict_fn}{Function for predicting the targets in the test folds/sets using the fitted model object.
 Will usually wrap \code{\link[stats:predict]{stats::predict()}}, but doesn't have to.

 Must have the following function arguments:

 \code{function(test_data, model,}

 \verb{         }\code{formula, hyperparameters)}

 Must return predictions in the following formats, depending on \code{type}:

 \subsection{Binomial}{
 Vector or one-column matrix / data frame with probabilities (0-1)
 of the second class, alphabetically.
 E.g.:

 \code{c(0.3, 0.5, 0.1, 0.5)}
 }

 \subsection{Gaussian}{
 Vector or one-column matrix / data frame with the predicted value.
 E.g.:

 \code{c(3.7, 0.9, 1.2, 7.3)}
 }

 \subsection{Multinomial}{
 Data frame with one column per class containing probabilities of the class.
 Column names should be identical to how the class names are written in the target column.
 E.g.:

 \tabular{rrr}{
  \strong{class_1} \tab \strong{class_2} \tab
  \strong{class_3} \cr
  0.269 \tab 0.528 \tab 0.203\cr
  0.368 \tab 0.322 \tab 0.310\cr
  0.375 \tab 0.371 \tab 0.254\cr
  ... \tab ... \tab ...}
 }}

\item{test_data}{Data frame. If specifying \code{partitions_col}, this can be \code{NULL}.}

\item{preprocess_fn}{Function for preprocessing the training and test sets.

 Can, for instance, be used to standardize both the training and test sets
 with the scaling and centering parameters from the training set.

 Must have the following function arguments:

 \code{function(train_data, test_data,}

 \verb{         }\code{formula, hyperparameters)}

 Must return a list with the preprocessed \code{train_data} and \code{test_data}. It may also contain
 a tibble with the \code{parameters} used in preprocessing:

 \code{list("train" = train_data,}

 \verb{     }\code{"test" = test_data,}

 \verb{     }\code{"parameters" = preprocess_parameters)}

 Additional elements in the returned list will be ignored.

 The optional parameters tibble will be included in the output.
 It could have the following format:

 \tabular{rrr}{
  \strong{Measure} \tab \strong{var_1} \tab \strong{var_2} \cr
  Mean \tab 37.921 \tab 88.231\cr
  SD \tab 12.4 \tab 5.986\cr
  ... \tab ... \tab ...}

 N.B. When \code{preprocess_once} is FALSE, the current formula and
 hyperparameters will be provided. Otherwise,
 these arguments will be \code{NULL}.}

\item{preprocess_once}{Whether to apply the preprocessing once
 (ignoring the formula and hyperparameters arguments in \code{preprocess_fn})
 or for every model separately. (Logical)

 When preprocessing does not depend on the current formula or hyperparameters,
 we can do the preprocessing of each train/test split once, to save time.
 This \strong{may require holding a lot more data in memory} though,
 why it is not the default setting.}

\item{hyperparameters}{Either a named list with hyperparameter values to combine in a grid
 or a data frame with one row per hyperparameter combination.

 \subsection{Named list for grid search}{
 Add \code{".n"} to sample the combinations. Can be the number of combinations to use,
 or a percentage between \code{0} and \code{1}.

 E.g.

 \code{list(".n" = 10,  # sample 10 combinations}

 \verb{     }\code{"lrn_rate" = c(0.1, 0.01, 0.001),}

 \verb{     }\code{"h_layers" = c(10, 100, 1000),}

 \verb{     }\code{"drop_out" = runif(5, 0.3, 0.7))}
 }

 \subsection{Data frame with specific hyperparameter combinations}{
 One row per combination to test.

 E.g.

 \tabular{rrr}{
  \strong{lrn_rate} \tab \strong{h_layers} \tab \strong{drop_out} \cr
  0.1 \tab 10 \tab 0.65\cr
  0.1 \tab 1000 \tab 0.65\cr
  0.01 \tab 1000 \tab 0.63\cr
  ... \tab ... \tab ...}
 }}

\item{partitions_col}{Name of grouping factor for identifying partitions. (Character)

 Rows with the value \code{1} in \code{partitions_col} are used as training set and
 rows with the value \code{2} are used as test set.

 N.B. \strong{Only used if \code{test_data} is \code{NULL}}.}

\item{cutoff}{Threshold for predicted classes. (Numeric)

 N.B. \strong{Binomial models only}}

\item{positive}{Level from dependent variable to predict.
 Either as character or level index (\code{1} or \code{2} - alphabetically).

 E.g. if we have the levels \code{"cat"} and \code{"dog"} and we want \code{"dog"} to be the positive class,
 we can either provide \code{"dog"} or \code{2}, as alphabetically, \code{"dog"} comes after \code{"cat"}.

 Used when calculating confusion matrix metrics and creating ROC curves.

 N.B. Only affects evaluation metrics, not the model training or returned predictions.

 N.B. \strong{Binomial models only}.}

\item{metrics}{List for enabling/disabling metrics.

  E.g. \code{list("RMSE" = FALSE)} would remove RMSE from the regression results,
  and \code{list("Accuracy" = TRUE)} would add the regular accuracy metric
  to the classification results.
  Default values (TRUE/FALSE) will be used for the remaining available metrics.

  You can enable/disable all metrics at once by including
  \code{"all" = TRUE/FALSE} in the list. This is done prior to enabling/disabling
  individual metrics, why f.i. \code{list("all" = FALSE, "RMSE" = TRUE)} would return only the RMSE metric.

  The list can be created with
  \code{\link[cvms:gaussian_metrics]{gaussian_metrics()}},
  \code{\link[cvms:binomial_metrics]{binomial_metrics()}}, or
  \code{\link[cvms:multinomial_metrics]{multinomial_metrics()}}.

  Also accepts the string \code{"all"}.}

\item{rm_nc}{Remove non-converged models from output. (Logical)}

\item{parallel}{Whether to cross-validate the list of models in parallel. (Logical)

 Remember to register a parallel backend first.
 E.g. with \code{doParallel::registerDoParallel}.}

\item{verbose}{Whether to message process information
like the number of model instances to fit. (Logical)}
}
\value{
Tbl (tibble) with the results and model objects.

 \subsection{Shared across families}{

 A nested tibble with \strong{coefficients} of the models. The coefficients
 are extracted from the model object with \code{\link[broom:tidy]{broom::tidy()}} or
 \code{\link[stats:coef]{coef()}} (with some restrictions on the output).
 If these attempts fail, a default coefficients tibble filled with \code{NA}s is returned.

 Nested tibble with the used \strong{preprocessing parameters},
 if a passed \code{preprocess_fn} returns the parameters in a tibble.

 Count of \strong{convergence warnings}, using a limited set of keywords (e.g. "convergence"). If a
 convergence warning does not contain one of these keywords, it will be counted with \strong{other warnings}.
 Consider discarding models that did not converge on all iterations.
 Note: you might still see results, but these should be taken with a grain of salt!

 Nested tibble with the \strong{warnings and messages} caught for each model.

 Specified \strong{family}.

 Nested \strong{model} objects.

 Name of \strong{dependent} variable.

 Names of \strong{fixed} effects.

 Names of \strong{random} effects, if any.

 }

 ----------------------------------------------------------------

 \subsection{Gaussian Results}{

 ----------------------------------------------------------------

 \strong{RMSE}, \strong{MAE}, \strong{NRMSE(IQR)},
 \strong{RRSE}, \strong{RAE}, \strong{RMSLE}.

 See the additional metrics (disabled by default) at \code{\link[cvms:gaussian_metrics]{?gaussian_metrics}}.

 A nested tibble with the \strong{predictions} and targets.
 }

 ----------------------------------------------------------------

 \subsection{Binomial Results}{

 ----------------------------------------------------------------

 Based on predictions of the test set,
 a confusion matrix and a ROC curve are created to get the following:

 ROC:

 \strong{AUC}, \strong{Lower CI}, and \strong{Upper CI}

 Confusion Matrix:

 \strong{Balanced Accuracy}, \strong{F1},
 \strong{Sensitivity}, \strong{Specificity},
 \strong{Positive Predictive Value},
 \strong{Negative Predictive Value},
 \strong{Kappa},
 \strong{Detection Rate},
 \strong{Detection Prevalence},
 \strong{Prevalence}, and
 \strong{MCC} (Matthews correlation coefficient).

 Other available metrics (disabled by default, see \code{metrics}):
 \strong{Accuracy}, \strong{AIC}, \strong{AICc}, \strong{BIC}.

 Also includes:

 A nested tibble with the \strong{predictions}, predicted classes (depends on \code{cutoff}), and targets.
 Note, that the \strong{predictions are not necessarily of the specified \code{positive} class}, but of
 the model's positive class (second level of dependent variable, alphabetically).

 A list of \strong{ROC} curve objects.

 A nested tibble with the \strong{confusion matrix}.
 The \code{Pos_} columns tells you whether a row is a
 True Positive (TP), True Negative (TN), False Positive (FP), or False Negative (FN),
 depending on which level is the "positive" class. I.e. the level you wish to predict.
 }

 ----------------------------------------------------------------

 \subsection{Multinomial Results}{

 ----------------------------------------------------------------

 For each class, a \emph{one-vs-all} binomial evaluation is performed. This creates
 a \strong{class level results} tibble containing the same metrics as the binomial results
 described above (excluding \code{MCC}, \code{AUC}, \code{Lower CI} and \code{Upper CI}), along with the \strong{Support} metric, which is simply a
 count of the class in the target column. These metrics are used to calculate the macro metrics
 in the output tibble. The nested class level results tibble is also included in the output tibble,
 and would usually be reported along with the macro and overall metrics.

 The output tibble contains the macro and overall metrics.
 The metrics that share their name with the metrics in the nested
 class level results tibble are averages of those metrics
 (note: does not remove \code{NA}s before averaging).
 In addition to these, it also includes the \strong{Overall Accuracy} and
 the multiclass \strong{MCC}.

 Other available metrics (disabled by default, see \code{metrics}):
 \strong{Accuracy}, multiclass \strong{AUC},  \strong{AIC}, \strong{AICc}, \strong{BIC},
 \strong{Weighted Balanced Accuracy}, \strong{Weighted Accuracy},
 \strong{Weighted F1}, \strong{Weighted Sensitivity}, \strong{Weighted Sensitivity},
 \strong{Weighted Specificity}, \strong{Weighted Pos Pred Value},
 \strong{Weighted Neg Pred Value}, \strong{Weighted Kappa},
 \strong{Weighted Detection Rate}, \strong{Weighted Detection Prevalence}, and
 \strong{Weighted Prevalence}.

 Note that the "Weighted" average metrics are weighted by the \code{Support}.

 Also includes:

 A nested tibble with the \strong{predictions}, predicted classes, and targets.

  A list of \strong{ROC} curve objects.

 A nested tibble with the multiclass \strong{Confusion Matrix}.

 \strong{Class Level Results}

 Besides the binomial evaluation metrics and the \code{Support} metric,
 the nested class level results tibble also includes:

 A nested tibble with the \strong{confusion matrix} from the one-vs-all evaluation.
 The \code{Pos_} columns tells you whether a row is a
 True Positive (TP), True Negative (TN), False Positive (FP), or False Negative (FN),
 depending on which level is the "positive" class. In our case, \code{1} is the current class
 and \code{0} represents all the other classes together.

 }
}
\description{
\Sexpr[results=rd, stage=render]{lifecycle::badge("experimental")}

 Fit your model function on a training set and validate it by
 predicting the test/validation set. Preprocess the train/test split.
 Validate different hyperparameter combinations and formulas at once.
 Returns results and fitted models in a tibble for easy reporting and further analysis.

 Compared to \code{\link[cvms:validate]{validate()}},
 this function allows you supply a custom model function, a predict function,
 a preprocess function and the hyperparameter values to validate.

 Supports regression and classification (binary and multiclass).
 See \code{type}.

 Note that some metrics may not be computable for some types
 of model objects.
}
\details{
Packages used:

 \subsection{Results}{
 \subsection{Shared}{

 AIC : \code{\link[stats:AIC]{stats::AIC}}

 AICc : \code{\link[MuMIn:AICc]{MuMIn::AICc}}

 BIC : \code{\link[stats:BIC]{stats::BIC}}

 }
 \subsection{Gaussian}{

 r2m : \code{\link[MuMIn:r.squaredGLMM]{MuMIn::r.squaredGLMM}}

 r2c : \code{\link[MuMIn:r.squaredGLMM]{MuMIn::r.squaredGLMM}}

 }
 \subsection{Binomial and Multinomial}{

 ROC and related metrics:

 Binomial: \code{\link[pROC:roc]{pROC::roc}}

 Multinomial: \code{\link[pROC:multiclass.roc]{pROC::multiclass.roc}}
 }
 }
}
\examples{
\donttest{
# Attach packages
library(cvms)
library(groupdata2) # fold()
library(dplyr) # \%>\% arrange() mutate()

# Note: More examples of custom functions can be found at:
# model_fn: example_model_functions()
# predict_fn: example_predict_functions()
# preprocess_fn: example_preprocess_functions()

# Data is part of cvms
data <- participant.scores

# Set seed for reproducibility
set.seed(7)

# Fold data
data <- partition(
  data,
  p = 0.8,
  cat_col = "diagnosis",
  id_col = "participant",
  list_out = FALSE
) \%>\%
  mutate(diagnosis = as.factor(diagnosis)) \%>\%
  arrange(.partitions)

# Formulas to validate

formula_gaussian <- "score ~ diagnosis"
formula_binomial <- "diagnosis ~ score"

#
# Gaussian
#

# Create model function that returns a fitted model object
lm_model_fn <- function(train_data, formula, hyperparameters) {
  lm(formula = formula, data = train_data)
}

# Create predict function that returns the predictions
lm_predict_fn <- function(test_data, model, formula, hyperparameters) {
  stats::predict(
    object = model,
    newdata = test_data,
    type = "response",
    allow.new.levels = TRUE
  )
}

# Validate the model function
v <- validate_fn(
  data,
  formulas = formula_gaussian,
  type = "gaussian",
  model_fn = lm_model_fn,
  predict_fn = lm_predict_fn,
  partitions_col = ".partitions"
)

v

# Extract model object
v$Model[[1]]

#
# Binomial
#

# Create model function that returns a fitted model object
glm_model_fn <- function(train_data, formula, hyperparameters) {
  glm(formula = formula, data = train_data, family = "binomial")
}

# Create predict function that returns the predictions
glm_predict_fn <- function(test_data, model, formula, hyperparameters) {
  stats::predict(
    object = model,
    newdata = test_data,
    type = "response",
    allow.new.levels = TRUE
  )
}

# Validate the model function
validate_fn(
  data,
  formulas = formula_binomial,
  type = "binomial",
  model_fn = glm_model_fn,
  predict_fn = glm_predict_fn,
  partitions_col = ".partitions"
)

#
# Support Vector Machine (svm)
# with known hyperparameters
#

# Create model function that returns a fitted model object
# We use the hyperparameters arg to pass in the kernel and cost values
# These will usually have been found with cross_validate_fn()
svm_model_fn <- function(train_data, formula, hyperparameters) {

  # Expected hyperparameters:
  #  - kernel
  #  - cost
  if (!"kernel" \%in\% names(hyperparameters))
    stop("'hyperparameters' must include 'kernel'")
  if (!"cost" \%in\% names(hyperparameters))
    stop("'hyperparameters' must include 'cost'")

  e1071::svm(
    formula = formula,
    data = train_data,
    kernel = hyperparameters[["kernel"]],
    cost = hyperparameters[["cost"]],
    scale = FALSE,
    type = "C-classification",
    probability = TRUE
  )
}

# Create predict function that returns the predictions
svm_predict_fn <- function(test_data, model, formula, hyperparameters) {
  predictions <- stats::predict(
    object = model,
    newdata = test_data,
    allow.new.levels = TRUE,
    probability = TRUE
  )

  # Extract probabilities
  probabilities <- dplyr::as_tibble(
    attr(predictions, "probabilities")
  )

  # Return second column
  probabilities[[2]]
}

# Specify hyperparameters to use
# We found these in the examples in ?cross_validate_fn()
svm_hparams <- list(
  "kernel" = "linear",
  "cost" = 10
)

# Validate the model function
validate_fn(
  data,
  formulas = formula_binomial,
  type = "binomial",
  model_fn = svm_model_fn,
  predict_fn = svm_predict_fn,
  hyperparameters = svm_hparams,
  partitions_col = ".partitions"
)
}
}
\seealso{
Other validation functions: 
\code{\link{cross_validate_fn}()},
\code{\link{cross_validate}()},
\code{\link{validate}()}
}
\author{
Ludvig Renbo Olsen, \email{r-pkgs@ludvigolsen.dk}
}
\concept{validation functions}
